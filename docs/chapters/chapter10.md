# 10. 无监督学习与聚类

## K均值聚类的推导

### 1. 目标函数

K-Means的目标是最小化所有簇的**簇内平方和** (Within-Cluster Sum of Squares, WCSS)。

设样本点为 $\{x_i\}$，簇的集合为 $\{C_k\}$，簇的质心为 $\{\mu_k\}$。目标函数 $J$ 为：

$$J = \sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2$$

### 2. 优化策略

这是一个NP-hard问题。我们采用**坐标下降法**，交替优化两个变量：

1. **簇分配 $C_k$**
2. **质心位置 $\mu_k$**

### 3. 推导算法两步

### 第一步：固定质心 $\mu_k$，优化簇分配 $C_k$

当所有质心 $\{\mu_k\}$ 固定时，为了让总的 $J$ 最小，我们只需对每个样本 $x_i$ 单独做决策，将其分配到能使其贡献 $\|x_i - \mu_k\|^2$ 最小的簇中。

因此，将每个 $x_i$ 分配给离它**最近**的质心 $\mu_k$ 对应的簇。

$$x_i \rightarrow \text{cluster } k, \quad \text{where } k = \arg\min_j \|x_i - \mu_j\|^2$$

这就是**分配步骤 (Assignment Step)**。

### 第二步：固定簇分配 $C_k$，优化质心 $\mu_k$

当簇分配 $\{C_k\}$ 固定时，目标函数 $J$ 可以分解为 $K$ 个独立的部分。我们对每个簇 $k$ 单独最小化其内部的平方和：

$$J_k = \sum_{x_i \in C_k} \|x_i - \mu_k\|^2$$

对 $\mu_k$ 求导并令其为0：

$$\frac{\partial J_k}{\partial \mu_k} = \sum_{x_i \in C_k} 2(\mu_k - x_i) = 0$$

解得：

$$\mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i$$

其中 $|C_k|$ 是簇 $C_k$ 中样本的数量。

结果是，最优的质心 $\mu_k$ 就是该簇所有样本点的**算术平均值**。
这就是**更新步骤 (Update Step)**。

### 算法复杂度

设 **n** 为样本数量，**K** 为簇的数量，**d** 为样本维度（特征数量），**t** 为迭代次数。

- **时间复杂度: $O(n \times K \times d \times t)$**
    - 每次迭代都包含两步：
        1. **分配步骤**: 对每个样本($n$)，计算其与 $K$ 个质心的距离（每个距离计算耗时 $d$），总计 $n \times K \times d$。
        2. **更新步骤**: 遍历所有样本($n$)一次，累加求和（耗时 $n \times d$），然后计算 $K$ 个质心的均值。
    - 主要耗时在分配步骤，因此单次迭代为 $O(K \times n \times d)$。
- **空间复杂度: $O(n \times d + K \times d)$**
    - 主要需要存储原始数据集($n \times d$)和 $K$ 个质心($K \times d$)。

由于 $t$, $K$, $d$ 通常远小于 $n$，K-Means 的复杂度可视为对数据量 $n$ 呈线性关系，因此非常高效。

---

### 优点

1. **简单高效**: 算法原理简单，易于实现，计算速度快，对大规模数据集友好。
2. **可解释性强**: 聚类结果以质心为代表，易于理解和解释。
3. **调参简单**: 主要参数仅有簇的数量 $K$。

---

### 缺点

1. **K值需要预先指定**: 必须手动设定簇的数量 $K$，但在很多场景下 $K$ 值是未知的。
2. **对初始质心敏感**: 随机选择的初始质心可能导致算法收敛到局部最优解，而非全局最优解。多次运行取最优结果可缓解此问题。
3. **对非球形簇效果不佳**: 算法基于距离度量，天然地假设簇是凸面且呈球状（各向同性），对非球形、细长或密度不均的簇识别效果差。
4. **对异常值敏感**: 异常值（Outliers）会显著影响质心的计算，从而影响最终的聚类结果。

## 模糊K均值聚类的推导

### 1. 目标函数

与K-Means类似，FCM也旨在最小化一个目标函数。但这次，每个样本点到簇中心的距离被其**隶属度的m次方**加权。

- 设 $\{x_i\}$ 为样本点，$i=1, ..., n$。
- 设 $\{c_k\}$ 为簇中心，$k=1, ..., K$。
- 设 $u_{ik}$ 为样本 $x_i$ 属于簇 $k$ 的**隶属度**，满足 $\sum_{k=1}^{K} u_{ik} = 1$。
- 设 $m$ 为**模糊化系数**（fuzzifier），通常取值为2。$m > 1$。

FCM的目标函数 $J_m$ 定义为：

$$J_m = \sum_{i=1}^{n} \sum_{k=1}^{K} (u_{ik})^m \|x_i - c_k\|^2$$

我们的目标是找到最优的隶属度矩阵 $U = \{u_{ik}\}$ 和簇中心 $C = \{c_k\}$ 来最小化 $J_m$。

### 2. 优化策略

同样采用**迭代优化**的策略。但由于有约束条件 $\sum_{k} u_{ik} = 1$，我们需要使用**拉格朗日乘子法**来推导隶属度的更新公式。

### 3. 推导算法两步

### 第一步：固定簇中心 $c_k$，优化隶属度 $u_{ik}$

固定簇中心，我们需要在约束 $\sum_{k=1}^{K} u_{ik} = 1$ 下最小化 $J_m$。为每个样本 $x_i$ 构建拉格朗日函数：

$$\mathcal{L}(u, \lambda) = \sum_{k=1}^{K} (u_{ik})^m \|x_i - c_k\|^2 - \lambda \left( \sum_{k=1}^{K} u_{ik} - 1 \right)$$

对 $u_{ik}$ 求偏导并令其为0：

$$\frac{\partial \mathcal{L}}{\partial u_{ik}} = m(u_{ik})^{m-1} \|x_i - c_k\|^2 - \lambda = 0$$

解出 $u_{ik}$:

$$u_{ik} = \left( \frac{\lambda}{m \|x_i - c_k\|^2} \right)^{\frac{1}{m-1}}$$

将此式代入约束条件 $\sum_{k=1}^{K} u_{ik} = 1$ 可以解出 $\lambda$，最终得到 $u_{ik}$ 的更新公式：

$$u_{ik} = \frac{1}{\sum_{j=1}^{K} \left( \frac{\|x_i - c_k\|}{\|x_i - c_j\|} \right)^{\frac{2}{m-1}}}$$

**直观解释**：一个样本对某个簇的隶属度，与它到该簇中心的距离成反比。距离越近，隶属度越高。

### 第二步：固定隶属度 $u_{ik}$，优化簇中心 $c_k$

固定隶属度，目标函数 $J_m$ 对每个簇中心 $c_k$ 是独立的。我们对 $c_k$ 求偏导并令其为0：

$$\frac{\partial J_m}{\partial c_k} = \sum_{i=1}^{n} (u_{ik})^m \cdot 2(c_k - x_i) = 0$$

整理后得到：

$$c_k \sum_{i=1}^{n} (u_{ik})^m = \sum_{i=1}^{n} (u_{ik})^m x_i$$

解出 $c_k$:

$$c_k = \frac{\sum_{i=1}^{n} (u_{ik})^m x_i}{\sum_{i=1}^{n} (u_{ik})^m}$$

**直观解释**：新的簇中心是所有样本点的**加权平均值**，权重是每个样本对该簇的隶属度的 $m$ 次方。隶属度越高的点，对中心点的“拉力”越大。

---

### 算法复杂度

- **时间复杂度: $O(n \times K \times d \times t)$**
    - 与K-Means同阶，但常数因子更大。
    - 更新隶属度矩阵需要计算每个点到所有中心的距离，复杂度为 $O(K \times n \times d)$。
    - 更新中心点需要遍历所有点计算加权平均，复杂度为 $O(n \times d \times K)$。
- **空间复杂度: $O(n \times K + n \times d)$**
    - 需要存储隶属度矩阵 $U$（大小为 $n \times K$）和数据集（大小为 $n \times d$），比K-Means需要更多的存储空间。

---

### 优点

1. **“软”聚类更灵活**: 隶属度的概念更符合许多现实场景，尤其是在簇边界模糊不清时，能提供更丰富的信息。
2. **对异常值鲁棒性更强**: 异常值通常会对所有簇有较低的隶属度，因此在计算簇中心时其影响被削弱，不像K-Means那样容易被单个异常点“带偏”。
3. **结果信息更丰富**: 提供了每个样本属于各个簇的概率，而不仅仅是一个分类标签。

---

### 缺点

1. **计算量更大**: 每次迭代的计算（尤其是隶属度更新）比K-Means更复杂，导致收敛速度较慢。
2. **需要指定K值和m值**: 不仅要预先指定簇数 $K$，还引入了模糊化系数 $m$，增加了调参的复杂度。
3. **对初始值敏感**: 和K-Means一样，FCM也可能陷入局部最优解，对初始中心点的选择敏感。
4. **仍然假设簇为球形**: 尽管是软聚类，其距离度量（欧氏距离）依然隐含了对球形簇的偏好。

## 问题：模糊K均值聚类与传统的K均值聚类有什么区别？解决了传统方法的什么问题？

模糊K-均值聚类（FCM）与传统K-均值（K-Means）最核心的区别在于**样本归属的定义方式**：

- **K-Means (硬聚类)**：每个样本点**唯一地、100%地**属于一个簇。
- **FCM (软聚类)**：每个样本点以一定的**隶属度（或概率）**同时属于所有簇。

这个核心区别解决了传统K-Means的两个主要问题：

1. **处理簇边界模糊的样本**: 对于处在两个簇边界之间的样本点，K-Means会强制将其划分给某一个簇，这可能不符合实际情况。FCM则允许它以较高的隶属度同时属于这两个簇，描述更加灵活和精确。
2. **降低对异常值的敏感度**: 在K-Means中，一个远离中心的异常值会被完全归入某个簇，并极大地影响该簇中心的计算。而在FCM中，异常值对所有簇的隶属度都会很低，因此在计算加权平均的簇中心时，其影响被显著削弱，使得算法更加鲁棒。

简单来说，FCM通过引入“模糊”的隶属度概念，使得聚类结果更能反映现实世界中类别界限不清晰的情况，并提升了算法的抗干扰能力。

## 问题：模糊K均值聚类中的m为什么要大于1，等于1行不行，为什么？

**$m$ 必须大于1，等于1时算法将退化为标准的K-均值（K-Means），失去“模糊”的意义。**

具体原因如下：

1. **当 $m = 1$ 时**:
在隶属度 $u_{ik}$ 的更新公式中，包含一个指数项 $\frac{2}{m-1}$。当 $m \to 1$ 时，这个指数趋向于无穷大。这会产生一个“赢家通吃”的效应：
    - 对于任何一个样本点，它到**最近**簇中心的距离比值会是1或小于1。
    - 经过无穷大次方的放大，只有到最近簇中心的隶属度会变成1，而到其他所有簇中心的隶属度都会变成0。
    这正是标准K-Means的“硬分配”规则，算法失去了模糊性。
2. **当 $m > 1$ 时**:
$m$ 作为**模糊化系数**（fuzzifier），它的值控制着聚类的“模糊”程度。
    - $m$ 越接近1（如1.1），聚类结果越接近“硬分配”，边界越清晰。
    - $m$ 越大（如2或3），不同簇之间的隶属度差异越小，聚类结果越模糊。

因此，为了保证算法的模糊特性，$m$ 必须大于1。在实践中，$m=2$ 是最常用的默认值。
