# 9. 独立于算法的机器学习

## NFL定理（No Free Lunch）

对于所有可能的问题，没有一种学习算法天生优于另一种。算法的性能高度依赖于具体问题的特性。这意味着不存在普遍"最好"的算法，选择算法时必须考虑问题的先验知识，如数据分布、任务本质等。

换句话说，即使是流行且有理论基础的算法，在某些学习算法与后验不“对齐或匹配”的问题上也会表现不佳。

## 丑小鸭定理（Ugly Duckling Theorem）

假设我们使用一组有限的谓词，这组谓词使我们能够区分所考虑的任意两种模式，那么任意两种这样的模式所共有的谓词数量是恒定的，且与这些模式的选择无关。此外，如果模式相似性是基于两种模式所共有的谓词总数，那么任意两种模式“同等相似”。

简而言之，在没有这些假设的情况下，既不存在本质上好的特征，也不存在本质上坏的特征；合适的特征取决于问题本身。


## **最小描述长度原理（Minimum Description Length, MDL）**

MDL旨在寻找能够最紧凑地描述数据的模型。它认为最优模型是使"模型自身的描述长度"加上"在该模型下数据的描述长度"之和最小的模型。这天然地惩罚了过于复杂的模型，是奥卡姆剃刀原理的一个形式化版本。


## **奥卡姆剃刀原理（Occam's Razor）**

内容是"如无必要，勿增实体"。在机器学习中，它建议我们选择能够很好地解释数据且本身最"简单"的模型。简单的模型通常被认为有更好的泛化能力，但NFL定理说明了这并非普遍真理，其有效性依赖于问题本身。


## **偏差与方差（Bias and Variance）**

偏差衡量的是模型的预测值与真实值之间的系统性差异（准不准），高偏差意味着"欠拟合"。方差衡量的是模型在不同训练集上的预测结果的变异性（稳不稳），高方差意味着"过拟合"。两者之间存在权衡关系。


## **两种重采样方法(独立于算法的算法)**

### 问题：**试给出两类提升学习泛化能力(且独立于算法的算法)的学习范式，简述之。**

**1. Boosting (增强法)**

这是一种串行学习范式。它通过迭代训练一系列“弱学习器”，每一个新的学习器都更关注前序学习器分错的样本（通过提升其权重）。最终，将所有弱学习器加权组合成一个强大的分类器，旨在显著提升整体的分类准确率。代表算法是AdaBoost。

**2. Bagging (自助聚合)**

这是一种并行学习范式。它通过从原始数据集中有放回地随机抽样，创建出多个不同的训练子集。然后，在每个子集上独立训练一个基学习器。最后，通过投票或平均的方式将所有学习器的结果进行组合。该方法能有效降低模型方差，提升不稳定模型的泛化能力。